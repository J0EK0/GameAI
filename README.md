# Game AI 專案總覽

本專案包含四款遊戲的 AI 設計與實作，目標為開發能夠在各遊戲環境中表現穩定、具備高勝率的自動化玩家。所有遊戲皆採用強化學習（Reinforcement Learning, RL）方法進行設計與決策邏輯開發，依據遊戲特性調整狀態特徵與行為策略，涵蓋從反應型控制到多目標任務的 AI 系統建構。

## 遊戲與對應 AI 方法

| 遊戲名稱         | 類型             | 控制方式         | AI 方法   |
|------------------|------------------|------------------|-----------|
| Ping Pong        | 即時反應          | 左右移動          | KNN        |
| Arkanoid         | 打磚塊            | 左右移動          | KNN        |
| Swimming Squid   | 生存與撿物導航     | 上下左右移動      | PPO        |
| Proly            | 3D 多目標導航決策  | 移動 + 朝向 + 道具 | PPO        |

## 實作內容

### Ping Pong

- 利用 KNN（K-Nearest Neighbors）分類模型 模擬人類反應，根據歷史資料推測球的未來落點並決定移動方向。
- 訓練資料涵蓋球的當前座標、速度向量、落點位置等特徵，分類目標為對應的控制動作（向左、向右或不動）。
- 模型可實現穩定接球並對對手變化球速或擊球角度做出合理反應，具備基本泛化能力。
- 資料前處理包含速度量化與落點分區，提升模型推論效率與精度。

### Arkanoid

- 使用 KNN 模型對球速、反彈角度與場上磚塊分布進行分類，預測合理接球位置與出擊方向。
- 模擬板子與球之間的互動行為，並針對特殊道具（如加速球、縮板、加分道具）進行 rule-based 決策處理。
- AI 可預測球在數次反彈後的落點，並將控制行為提早排程，減少錯失接球機率。
- 訓練資料來自實際對戰過程紀錄，並透過區間劃分簡化狀態空間。

### Swimming Squid

- 採用 PPO（Proximal Policy Optimization）強化學習演算法 訓練代理人，讓其在隨機生成的障礙與加分物環境中學會導航、生存與撿物。
- 特徵提取方式為固定方向矩形感測區域（上下左右），統計各區域中加分物、扣分物與障礙物的加權距離總和，並進行量化處理。
- Reward 設計包含以下面向：
  - 分數變化（正向獎勵／懲罰）
  - 對行為方向正確性的加分（如向加分物移動）
  - 存活時間、移動效率與牆邊懲罰等
- 模型經過多輪訓練後，展現出穩定的生存策略與撿物能力，並可適應不同場景與物件分布。

### Proly

- 初始版本實作 rule-based 向量導引策略，將目標導引、敵人迴避與撿物優先權向量加權合成決策方向。
- PPO 模型訓練階段中，觀察空間包含自身位置與朝向、目標方向、敵人方向與距離、可撿道具資訊等。
- 策略空間涵蓋移動指令、朝向控制與道具選擇等行為。
- Reward 設計以任務完成度（如撿物成功、避開敵人、存活時間）為主，搭配動作穩定性與效率指標。
- 訓練流程包含 baseline 初始化、訓練環境隨機化、動作平滑正則等強化學習技巧。

## 專案資料夾結構

```
GameAI-Project/
├── pingpong/ # KNN AI for Ping Pong
├── arkanoid/ # KNN AI for Arkanoid
├── squid/ # PPO AI for Swimming Squid
├── proly/ # PPO AI for Proly
└── README.md
```
